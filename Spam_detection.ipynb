{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo9TiM3sdCTo+krKJbi4Pw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nanditha09/CleanInboxAI/blob/main/Spam_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# Rename columns for clarity\n",
        "df = df.rename(columns={'v1': 'label', 'v2': 'message'})\n",
        "\n",
        "# Drop all columns with names starting with 'Unnamed:'\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "\n",
        "# Add numerical labels for 'ham' and 'spam'\n",
        "df['label_num'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' ', text, flags=re.MULTILINE)\n",
        "    # Remove phone numbers\n",
        "    text = re.sub(r'\\b\\d{10}\\b', ' ', text)\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    # Lemmatize words\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing\n",
        "df['clean_message'] = df['message'].apply(preprocess_text)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df.head())\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the vectorizer to capture unigrams, bigrams, and trigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
        "\n",
        "# Fit and transform your cleaned messages\n",
        "X_tfidf = vectorizer.fit_transform(df['clean_message'])\n",
        "\n",
        "# Save the fitted vectorizer\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Detect presence of URLs\n",
        "df['has_url'] = df['message'].apply(lambda x: int(bool(re.search(r'http[s]?://|www\\.', x))))\n",
        "\n",
        "# Detect presence of email addresses\n",
        "df['has_email'] = df['message'].apply(lambda x: int(bool(re.search(r'\\S+@\\S+', x))))\n",
        "\n",
        "# Detect presence of phone numbers (simple pattern)\n",
        "df['has_phone'] = df['message'].apply(lambda x: int(bool(re.search(r'\\b\\d{10,}\\b', x))))\n",
        "\n",
        "# Combine TF-IDF features with the new binary features\n",
        "X_additional = np.array(df[['has_url', 'has_email', 'has_phone']])\n",
        "X_combined = hstack([X_tfidf, X_additional])\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Prepare target variable\n",
        "y = df['label_num']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost classifier\n",
        "model = XGBClassifier(eval_metric='logloss')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print accuracy and classification report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Save the trained model and vectorizer\n",
        "joblib.dump(model, 'xgb_spam_model.pkl')\n",
        "joblib.dump(vectorizer, 'vectorizer.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEyRF8Z2UqTd",
        "outputId": "308d09d7-9c26-4a4c-df46-684e829565fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message  label_num  \\\n",
            "0   ham  Go until jurong point, crazy.. Available only ...          0   \n",
            "1   ham                      Ok lar... Joking wif u oni...          0   \n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1   \n",
            "3   ham  U dun say so early hor... U c already then say...          0   \n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...          0   \n",
            "\n",
            "                                       clean_message  \n",
            "0  go jurong point crazy available bugis n great ...  \n",
            "1                            ok lar joking wif u oni  \n",
            "2  free entry wkly comp win fa cup final tkts st ...  \n",
            "3                u dun say early hor u c already say  \n",
            "4                nah think go usf life around though  \n",
            "Accuracy: 0.9811659192825112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Ham       0.98      1.00      0.99       965\n",
            "        Spam       0.97      0.89      0.93       150\n",
            "\n",
            "    accuracy                           0.98      1115\n",
            "   macro avg       0.98      0.94      0.96      1115\n",
            "weighted avg       0.98      0.98      0.98      1115\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "# Load trained model and vectorizer\n",
        "model = joblib.load('xgb_spam_model.pkl')\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "# Preprocessing function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' ', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\b\\d{10,}\\b', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"ðŸ“© Spam Detection App\")\n",
        "st.write(\"Enter a message to classify it as Spam or Not Spam.\")\n",
        "\n",
        "# Input box\n",
        "message = st.text_area(\"Your Message\", \"\")\n",
        "\n",
        "# Prediction\n",
        "if st.button(\"Predict\"):\n",
        "    if message.strip() == \"\":\n",
        "        st.warning(\"Please enter a message.\")\n",
        "    else:\n",
        "        processed_message = preprocess_text(message)\n",
        "        data_tfidf = vectorizer.transform([processed_message])\n",
        "\n",
        "        # Detect presence of URLs, email addresses, and phone numbers\n",
        "        has_url = int(bool(re.search(r'http[s]?://|www\\.', message)))\n",
        "        has_email = int(bool(re.search(r'\\S+@\\S+', message)))\n",
        "        has_phone = int(bool(re.search(r'\\b\\d{10,}\\b', message)))\n",
        "\n",
        "        # Combine TF-IDF features with the new binary features\n",
        "        additional_features = np.array([[has_url, has_email, has_phone]])\n",
        "        data_combined = hstack([data_tfidf, additional_features])\n",
        "\n",
        "        prediction = model.predict(data_combined)[0]\n",
        "        if prediction == 1:\n",
        "            st.error(\"ðŸš¨ This is SPAM!\")\n",
        "        else:\n",
        "            st.success(\"âœ… This is NOT spam.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTvbVT6lUwRX",
        "outputId": "1879574c-27e4-4b32-bdbb-2ffaea05e87a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 20:28:53.955 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.126 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n",
            "2025-04-17 20:28:54.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.135 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.136 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.138 Session state does not function when running a script without `streamlit run`\n",
            "2025-04-17 20:28:54.139 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-04-17 20:28:54.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Streamlit app code to a file\n",
        "app_code = '''import streamlit as st\n",
        "import joblib\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "\n",
        "# Load trained model and vectorizer\n",
        "model = joblib.load('xgb_spam_model.pkl')\n",
        "vectorizer = joblib.load('vectorizer.pkl')\n",
        "\n",
        "# Preprocessing function\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' ', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\b\\d{10,}\\b', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"ðŸ“© Spam Detection App\")\n",
        "st.write(\"Enter a message to classify it as Spam or Not Spam.\")\n",
        "\n",
        "# Input box\n",
        "message = st.text_area(\"Your Message\", \"\")\n",
        "\n",
        "# Prediction\n",
        "if st.button(\"Predict\"):\n",
        "    if message.strip() == \"\":\n",
        "        st.warning(\"Please enter a message.\")\n",
        "    else:\n",
        "        processed_message = preprocess_text(message)\n",
        "        data_tfidf = vectorizer.transform([processed_message])\n",
        "\n",
        "        # Detect presence of URLs, email addresses, and phone numbers\n",
        "        has_url = int(bool(re.search(r'http[s]?://|www\\.', message)))\n",
        "        has_email = int(bool(re.search(r'\\S+@\\S+', message)))\n",
        "        has_phone = int(bool(re.search(r'\\b\\d{10,}\\b', message)))\n",
        "\n",
        "        # Combine TF-IDF features with the new binary features\n",
        "        additional_features = np.array([[has_url, has_email, has_phone]])\n",
        "        data_combined = hstack([data_tfidf, additional_features])\n",
        "\n",
        "        prediction = model.predict(data_combined)[0]\n",
        "        if prediction == 1:\n",
        "            st.error(\"ðŸš¨ This is SPAM!\")\n",
        "        else:\n",
        "            st.success(\"âœ… This is NOT spam.\")\n",
        "'''\n",
        "\n",
        "with open(\"app.py\", \"w\") as file:\n",
        "    file.write(app_code)\n",
        "\n",
        "# Allow downloading the app\n",
        "from google.colab import files\n",
        "files.download(\"app.py\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Q6mmPTzTU0pn",
        "outputId": "c68a4f25-28fd-421c-b710-3a58962f482b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5ff45416-9340-4cd8-9172-edd9c213afb6\", \"app.py\", 1934)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip freeze | sed 's/==.*$//' > requirements.txt\n"
      ],
      "metadata": {
        "id": "SiOG07E5VWRs"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}